{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pathjoin\n",
    "import sys\n",
    "sys.path.insert(0, '/home/mlepekhin/Non-thematic-Text-Classification/code/allennlp_experiments')\n",
    "from data_processing import *\n",
    "from interpretation import *\n",
    "from models import *\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/mlepekhin/models/competition’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir /home/mlepekhin/models/competition\n",
    "\n",
    "\n",
    "DATA_DIR = '/home/mlepekhin/data'\n",
    "MODELS_DIR = '/home/mlepekhin/models/competition'\n",
    "MAX_TOKENS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_ID1 = 'bert_en_pretrain' \n",
    "MODEL_ID1 = 'bert_fr_it_en_news_finetune'\n",
    "CHECKPOINTS_DIR1 = pathjoin(MODELS_DIR, MODEL_ID1, 'checkpoints')\n",
    "BEST_MODEL1 = pathjoin(CHECKPOINTS_DIR1, 'best.th')\n",
    "transformer_model1 = 'bert-base-multilingual-cased'\n",
    "model_dir1 = pathjoin(MODELS_DIR, MODEL_ID1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID2 = 'xlm_roberta_fr_it_en_finetune' \n",
    "CHECKPOINTS_DIR2 = pathjoin(MODELS_DIR, MODEL_ID2, 'checkpoints')\n",
    "BEST_MODEL2 = pathjoin(CHECKPOINTS_DIR2, 'best.th')\n",
    "transformer_model2 = 'xlm-roberta-base'\n",
    "#model_dir2 = '/home/mlepekhin/competition/all_finetune_en_xlm_roberta_genres'\n",
    "model_dir2 = '/home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.txt  non_padded_namespaces.txt  tags.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/mlepekhin/models/xlm_roberta_en_ftd_seed42/vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'model_dir2': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls model_dir2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best.th  log\r\n"
     ]
    }
   ],
   "source": [
    "!ls {model_dir2}/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:38:56|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "02272023 10:38:56|INFO|filelock| Lock 140610251442832 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:38:57|INFO|filelock| Lock 140610251442832 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:38:57|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab.\n",
      "02272023 10:38:57|INFO|filelock| Lock 140610251443312 acquired on /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab/.lock\n",
      "02272023 10:38:57|INFO|filelock| Lock 140610251443312 released on /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab/.lock\n"
     ]
    }
   ],
   "source": [
    "vocab1 = Vocabulary.from_files(pathjoin(MODELS_DIR, MODEL_ID1, 'vocab'))\n",
    "vocab2 = Vocabulary.from_files(pathjoin(MODELS_DIR, MODEL_ID2, 'vocab'))\n",
    "#vocab2 = Vocabulary.from_files(pathjoin('/home/mlepekhin/competition/all_finetune_en_xlm_roberta_genres', 'vocab'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 = build_transformer_model(vocab1, transformer_model1)\n",
    "#model1.load_state_dict(torch.load(BEST_MODEL1, map_location = 'cuda:0'))\n",
    "#model1.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 = build_pool_transformer_model(vocab2, transformer_model2)\n",
    "#model2.load_state_dict(torch.load(BEST_MODEL2, map_location = 'cuda:0'))\n",
    "#model2.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:38:58|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:38:58|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:38:58|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:38:59|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:38:59|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:38:59|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:39:00|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:00|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:00|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:39:01|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:01|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:02|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:39:02|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:02|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:03|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "02272023 10:39:04|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:04|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:04|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:05|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:05|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:06|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "02272023 10:39:07|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:07|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:08|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = PretrainedTransformerTokenizer(transformer_model1)\n",
    "indexer1 = PretrainedTransformerIndexer(transformer_model1)\n",
    "\n",
    "\n",
    "tokenizer2 = PretrainedTransformerTokenizer(transformer_model2)\n",
    "indexer2 = PretrainedTransformerIndexer(transformer_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_transformer_classifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:08|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "02272023 10:39:08|INFO|filelock| Lock 140610251443936 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:39:09|INFO|filelock| Lock 140610251443936 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:09|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:09|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:09|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "02272023 10:39:13|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:13|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:14|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:39:14|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:14|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:15|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:39:16|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:16|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:16|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "02272023 10:39:23|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:23|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:24|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:39:24|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:24|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:25|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:25|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:25|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:26|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:39:26|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:26|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:27|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:39:30|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "02272023 10:39:30|INFO|filelock| Lock 140624274444000 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:39:30|INFO|filelock| Lock 140624274444000 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n"
     ]
    }
   ],
   "source": [
    "dev_text_list = pd.read_csv('en_dev.csv').text.values\n",
    "dev_ids = pd.read_csv('en_dev.csv').article_id.values\n",
    "cuda_device = 1 \n",
    "    \n",
    "probs1 = get_transformer_model_prob_predictions(transformer_model1, 'en_dev.csv', model_dir1,\n",
    "                                      8, cuda_device=cuda_device, use_bert_pooler=True,\n",
    "                                      verbose=True, probs_filename=None, dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:30|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab.\n",
      "02272023 10:39:30|INFO|filelock| Lock 140624557206112 acquired on /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab/.lock\n",
      "02272023 10:39:30|INFO|filelock| Lock 140624557206112 released on /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab/.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:31|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:31|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:31|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267\n",
      "02272023 10:39:43|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:43|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:43|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "02272023 10:39:44|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:44|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:45|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "02272023 10:39:47|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:47|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:47|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "02272023 10:39:48|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:48|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:49|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "02272023 10:39:50|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:50|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "02272023 10:39:51|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "02272023 10:39:52|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "02272023 10:39:52|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:52|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (709 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:53|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (677 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (741 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (684 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (712 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:54|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (708 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (709 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (725 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (804 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (782 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (702 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (763 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:55|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (767 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (784 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (759 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (741 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (679 > 512). Running this sequence through the model will result in indexing errors\n",
      "02272023 10:39:56|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab.\n",
      "02272023 10:39:56|INFO|filelock| Lock 140624274305904 acquired on /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab/.lock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:56|INFO|filelock| Lock 140624274305904 released on /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab/.lock\n"
     ]
    }
   ],
   "source": [
    "probs2 = get_transformer_model_prob_predictions(transformer_model2, 'en_dev.csv', model_dir2,\n",
    "                                      8, cuda_device=cuda_device, use_bert_pooler=False,\n",
    "                                      verbose=True, probs_filename=None, dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:56|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "02272023 10:39:56|INFO|filelock| Lock 140621688292736 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:39:57|INFO|filelock| Lock 140621688292736 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:39:57|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:39:57|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:39:57|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "02272023 10:40:01|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:01|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:01|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:02|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:02|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:03|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:03|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:03|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:03|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "02272023 10:40:08|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:08|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:08|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:09|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:09|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:09|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:40:10|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:10|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:11|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:11|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:11|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:12|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:15|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "02272023 10:40:15|INFO|filelock| Lock 140624273944336 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:40:15|INFO|filelock| Lock 140624273944336 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:40:15|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "02272023 10:40:15|INFO|filelock| Lock 140624417041904 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:40:15|INFO|filelock| Lock 140624417041904 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:40:16|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:16|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:16|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "02272023 10:40:19|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:19|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:20|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:20|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:20|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:21|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:22|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:22|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:22|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "02272023 10:40:26|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:26|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:27|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:27|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:27|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:28|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:40:28|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:28|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:29|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:29|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:29|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:30|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:33|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "02272023 10:40:33|INFO|filelock| Lock 140624274417744 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:40:33|INFO|filelock| Lock 140624274417744 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:40:33|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "02272023 10:40:33|INFO|filelock| Lock 140624272994112 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "02272023 10:40:33|INFO|filelock| Lock 140624272994112 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:40:34|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:34|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:34|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "02272023 10:40:37|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:37|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:38|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:39|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:39|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:39|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "02272023 10:40:40|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "02272023 10:40:40|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02272023 10:40:40|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-156a3c07acd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     cur_probs = get_transformer_model_prob_predictions(transformer_model1, 'en_dev.csv', model_dir1,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                       \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bert_pooler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                       verbose=True, probs_filename=None, dropout=True)\n",
      "\u001b[0;32m~/Non-thematic-Text-Classification/code/allennlp_experiments/evaluate_transformer_classifier.py\u001b[0m in \u001b[0;36mget_transformer_model_prob_predictions\u001b[0;34m(transformer_model, test_data_filename, model_dir, batch_size, cuda_device, use_bert_pooler, verbose, probs_filename, dropout)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# load the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedTransformerTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlepekhin_research/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conf_probs1 = []\n",
    "\n",
    "for _ in range(10):\n",
    "    cur_probs = get_transformer_model_prob_predictions(transformer_model1, 'en_dev.csv', model_dir1,\n",
    "                                      8, cuda_device=cuda_device, use_bert_pooler=True,\n",
    "                                      verbose=True, probs_filename=None, dropout=True)\n",
    "    conf_probs1.append(cur_probs[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_probs2 = []\n",
    "\n",
    "for _ in range(10):\n",
    "    cur_probs = get_transformer_model_prob_predictions(transformer_model2, 'en_dev.csv', model_dir2,\n",
    "                                      8, cuda_device=cuda_device, use_bert_pooler=False,\n",
    "                                      verbose=True, probs_filename=None, dropout=True)\n",
    "    conf_probs2.append(cur_probs[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf1 = np.mean(conf_probs1, axis=0).max(axis=-1)\n",
    "conf2 = np.mean(conf_probs2, axis=0).max(axis=-1)\n",
    "conf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(conf1, probs1.max(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(conf2, probs2.max(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = ['reporting', 'opinion', 'satire']\n",
    "dev_targets = pd.read_csv('en_dev.csv').target.values\n",
    "\n",
    "\n",
    "good_ids1 = np.array([id_to_label[p] for p in probs1.argmax(axis=-1)]) == dev_targets\n",
    "good_ids2 = np.array([id_to_label[p] for p in probs2.argmax(axis=-1)]) == dev_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(conf1[good_ids1], probs1[good_ids1].max(axis=-1)))\n",
    "print(np.corrcoef(conf1[~good_ids1], probs1[~good_ids1].max(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(conf2[good_ids2], probs2[good_ids2].max(axis=-1)))\n",
    "print(np.corrcoef(conf2[~good_ids2], probs2[~good_ids2].max(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.mean(np.array(conf_probs1) * 0.65 + np.array(conf_probs2) * 0.35, axis=0).max(axis=-1)\n",
    "threshold = np.percentile(conf, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_probs = probs1 * 0 + probs2 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = ['reporting', 'opinion', 'satire']\n",
    "ensemble_predictions = []\n",
    "\n",
    "with open('en_ensemble2_0.65_dev_prediction_threshold_20.txt', 'w') as fout:\n",
    "    for dev_id, ensemble_prob in zip(dev_ids, ensemble_probs):\n",
    "        #if max(ensemble_prob) <= threshold:\n",
    "        #    fout.write(f'{dev_id}\\treporting\\n')\n",
    "        #    continue\n",
    "            \n",
    "        label_id = np.argmax(ensemble_prob)\n",
    "        label = id_to_label[label_id]\n",
    "        ensemble_predictions.append(label)\n",
    "        \n",
    "        fout.write(f'{dev_id}\\t{label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['opinion', 'reporting', 'satire'], dtype=object), array([20, 54,  9]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(targets, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = np.array([id_to_label[np.argmax(prob)] for prob in probs1])\n",
    "labels2 = np.array([id_to_label[np.argmax(prob)] for prob in probs2])\n",
    "targets = pd.read_csv('en_dev.csv').target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall max p: 0.8677507958498346\n",
      "correct max p: 0.9111060398189645\n",
      "incorrect max p: 0.8311397009425693\n",
      "percentile 10 max p: 0.6924393773078918\n"
     ]
    }
   ],
   "source": [
    "print('overall max p:', np.max(probs1, axis=-1).mean())\n",
    "print('correct max p:', np.max(probs1[labels1 == targets], axis=-1).mean())\n",
    "print('incorrect max p:', np.max(probs1[labels1 != targets], axis=-1).mean())\n",
    "print('percentile 10 max p:', np.percentile(np.max(probs1, axis=-1), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall max p: 0.9002301412892629\n",
      "correct max p: 0.901768125806536\n",
      "incorrect max p: 0.8574666840334734\n",
      "percentile 10 max p: 0.7315416932106018\n"
     ]
    }
   ],
   "source": [
    "print('overall max p:', np.max(probs2, axis=-1).mean())\n",
    "print('correct max p:', np.max(probs2[labels2 == targets], axis=-1).mean())\n",
    "print('incorrect max p:', np.max(probs1[labels2 != targets], axis=-1).mean())\n",
    "print('percentile 10 max p:', np.percentile(np.max(probs2, axis=-1), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['opinion', 'reporting', 'satire'], dtype=object), array([20, 54,  9]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(targets, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels1 = sorted(np.unique(pd.read_csv('/home/mlepekhin/data/en_train').target.values))\n",
    "labels1 = ['reporting', 'opinion', 'satire']\n",
    "labels2 = ['reporting', 'opinion', 'satire']\n",
    "all_labels = labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'reporting',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'opinion',\n",
       " 'reporting',\n",
       " 'reporting',\n",
       " 'reporting',\n",
       " 'opinion',\n",
       " 'opinion']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#print(conf[ensemble_predictions == targets].mean(), conf[ensemble_predictions != targets].mean())\n",
    "ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02272023 10:42:03|INFO|numexpr.utils| Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "02272023 10:42:03|INFO|numexpr.utils| NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7fe501776dc0>,\n",
       "  <matplotlib.axis.XTick at 0x7fe501776d90>,\n",
       "  <matplotlib.axis.XTick at 0x7fe5ac9957f0>],\n",
       " [Text(0.5, 0, 'reporting'), Text(1.5, 0, 'opinion'), Text(2.5, 0, 'satire')])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(15, 15)) \n",
    "cm = confusion_matrix(pd.read_csv('en_dev.csv').target.values, ensemble_predictions, labels=all_labels)[:11, -3:]\n",
    "sums = np.sum(cm, axis=1)\n",
    "\n",
    "normed_cm = (cm.T / sums).T\n",
    "sns.heatmap(normed_cm, annot=True, fmt='.2%')\n",
    "plt.yticks(0.5 + np.arange(len(labels1)), labels=labels1, fontsize=12)\n",
    "plt.xticks(0.5 + np.arange(len(labels2)), labels=labels2, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01312023 19:08:22|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "01312023 19:08:22|INFO|filelock| Lock 139807930450752 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "01312023 19:08:22|INFO|filelock| Lock 139807930450752 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01312023 19:08:23|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "01312023 19:08:23|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "01312023 19:08:23|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "01312023 19:08:27|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "01312023 19:08:27|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "01312023 19:08:27|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "01312023 19:08:28|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "01312023 19:08:28|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "01312023 19:08:28|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "01312023 19:08:29|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "01312023 19:08:29|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "01312023 19:08:29|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "01312023 19:08:36|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "01312023 19:08:36|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "01312023 19:08:36|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "01312023 19:08:37|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "01312023 19:08:37|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "01312023 19:08:37|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01312023 19:08:38|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "01312023 19:08:38|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "01312023 19:08:39|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "01312023 19:08:39|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/mlepekhin/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "01312023 19:08:39|INFO|transformers.configuration_utils| Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "01312023 19:08:40|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/mlepekhin/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "01312023 19:08:42|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab.\n",
      "01312023 19:08:42|INFO|filelock| Lock 139807193492736 acquired on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "01312023 19:08:42|INFO|filelock| Lock 139807193492736 released on /home/mlepekhin/models/competition/bert_fr_it_en_news_finetune/vocab/.lock\n",
      "01312023 19:08:42|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab.\n",
      "01312023 19:08:42|INFO|filelock| Lock 139807193421520 acquired on /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab/.lock\n",
      "01312023 19:08:42|INFO|filelock| Lock 139807193421520 released on /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab/.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01312023 19:08:43|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01312023 19:08:43|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01312023 19:08:43|INFO|transformers.modeling_utils| loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /home/mlepekhin/.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267\n",
      "01312023 19:08:54|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01312023 19:08:54|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01312023 19:08:54|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01312023 19:08:55|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01312023 19:08:55|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01312023 19:08:56|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01312023 19:08:58|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01312023 19:08:58|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01312023 19:08:59|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01312023 19:09:00|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01312023 19:09:00|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01312023 19:09:00|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01312023 19:09:01|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01312023 19:09:01|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "01312023 19:09:02|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01312023 19:09:03|INFO|transformers.configuration_utils| loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /home/mlepekhin/.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "01312023 19:09:03|INFO|transformers.configuration_utils| Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01312023 19:09:03|INFO|transformers.tokenization_utils| loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /home/mlepekhin/.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "01312023 19:09:04|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:04|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (765 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:04|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:04|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:04|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (704 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:04|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:04|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:04|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (743 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (703 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (709 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:05|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (703 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (734 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (740 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (783 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|WARNING|transformers.tokenization_utils| Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "01312023 19:09:06|INFO|allennlp.data.vocabulary| Loading token dictionary from /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab.\n",
      "01312023 19:09:06|INFO|filelock| Lock 139807193420992 acquired on /home/mlepekhin/models/competition/xlm_roberta_fr_it_en_finetune/vocab/.lock\n"
     ]
    }
   ],
   "source": [
    "dev_text_list = pd.read_csv('en_test.csv').text.values\n",
    "dev_ids = pd.read_csv('en_test.csv').article_id.values\n",
    "cuda_device = 0 \n",
    "    \n",
    "probs1 = get_transformer_model_prob_predictions(transformer_model1, 'en_test.csv', model_dir1,\n",
    "                                      8, cuda_device=cuda_device, use_bert_pooler=True,\n",
    "                                      verbose=True, probs_filename=None, dropout=False)\n",
    "\n",
    "probs2 = get_transformer_model_prob_predictions(transformer_model2, 'en_test.csv', model_dir2,\n",
    "                                      8, cuda_device=cuda_device, use_bert_pooler=False,\n",
    "                                      verbose=True, probs_filename=None, dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ensemble_probs = probs1 * 0.65 + probs2 * 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = ['reporting', 'opinion', 'satire']\n",
    "ensemble_predictions = []\n",
    "\n",
    "with open('en_ensemble2_0.65_test_prediction_major_threshold_20.txt', 'w') as fout:\n",
    "    for dev_id, ensemble_prob in zip(dev_ids, ensemble_probs):\n",
    "        if max(ensemble_prob) <= threshold:\n",
    "            fout.write(f'{dev_id}\\treporting\\n')\n",
    "            continue\n",
    "            \n",
    "        label_id = np.argmax(ensemble_prob)\n",
    "        label = id_to_label[label_id]\n",
    "        ensemble_predictions.append(label)\n",
    "        \n",
    "        fout.write(f'{dev_id}\\t{label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
